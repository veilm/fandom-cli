Short answer: yes—Fandom wikis expose a **MediaWiki API** that lets you enumerate every file (originals, not just thumbnails), and there are also “special pages” and sitemaps you can use to crawl pages if you want to mine galleries or specific categories. Below is a practical, rate-friendly blueprint you can run over weeks.

# The cleanest way: enumerate every file via the API

Fandom runs on MediaWiki, so each subdomain (e.g., `rezero.fandom.com`) has `api.php`. The **`list=allimages`** module will page through every uploaded file on that wiki; pair it with **`prop=imageinfo`** to get the original download URL.

**Docs to rely on**

* `list=allimages` (enumerate all files). ([MediaWiki][1])
* `prop=imageinfo` (get original URL, size, mime, etc.). ([MediaWiki][2])
* Fandom dev page noting MediaWiki API usage on Fandom. ([dev.fandom.com][3])

**Minimal, respectful crawler (Python)**

```python
import requests, time, pathlib

S = requests.Session()
API = "https://rezero.fandom.com/api.php"

params = {
  "action": "query",
  "format": "json",
  "list": "allimages",
  "ailimit": "500",                 # 500 per request (max for users); 5000 if you use a bot account
  "aiprop": "url|mime|size|sha1"    # include original file URL + metadata
}

cont = {}
out = pathlib.Path("rezero-files")
out.mkdir(exist_ok=True)

while True:
    r = S.get(API, params={**params, **cont}, timeout=60)
    r.raise_for_status()
    data = r.json()
    for f in data["query"]["allimages"]:
        url = f["url"]               # original file (on static.wikia.nocookie.net)
        name = f'{f["sha1"]}_{f["name"].replace(" ", "_")}'
        to = out / name
        if not to.exists():
            with S.get(url, stream=True, timeout=120) as resp:
                resp.raise_for_status()
                with open(to, "wb") as fh:
                    for chunk in resp.iter_content(1 << 15):
                        fh.write(chunk)
        time.sleep(0.8)              # be gentle: ~1 req/sec
    if "continue" not in data:
        break
    cont = data["continue"]
    time.sleep(2)                    # small pause between pages
```

That will yield **every original image** hosted for that wiki (they live on `static.wikia.nocookie.net`, a cookie-less asset host Fandom uses). ([community.fandom.com][4])

# If you only want “official art”: target categories

Most wikis keep “official” images in specific categories (or subcats) and/or per-character *Image Gallery* pages. You can pull **only files in a category**:

* Use `list=categorymembers` with `cmtitle=Category:<Name>&cmtype=file` to list just files in that category, then feed those titles into `prop=imageinfo` to get the original URLs. ([MediaWiki][5])

Example (titles from a category → original URLs):

```
GET https://rezero.fandom.com/api.php
  ?action=query&list=categorymembers
  &cmtitle=Category:Image_Gallery
  &cmtype=file&cmlimit=500&format=json
```

Then for the returned `File:*.png` titles:

```
GET https://rezero.fandom.com/api.php
  ?action=query&prop=imageinfo
  &titles=File:Example.png|File:Another.jpg
  &iiprop=url|mime|size&format=json
```

(The Re:Zero wiki has a top-level **Image Gallery** category that branches to loads of character/game galleries you can mine systematically.) ([Re:Zero Wiki][6])

# Crawling pages (if you prefer to harvest from gallery pages)

If you want to walk *pages* and scrape gallery sections:

* **Special pages to list content:**

  * `Special:AllPages` — all pages on a wiki (useful to discover `/Image_Gallery` pages). ([community.fandom.com][7])
  * `Special:ListFiles` — a paginated list of all uploaded files. ([Super User][8])
* **Sitemaps**: many Fandom wikis expose sitemap indexes at `/sitemap-newsitemapxml-index.xml`, which you can stream to discover every content page and then use `prop=images`/`prop=imageinfo` to extract embeds. (Example sitemap index on another Fandom wiki shown here.) ([Peaky Blinders Wiki][9])

# Ready-made scripts (if you don’t want to code from scratch)

* **ListFiles** (on-wiki/userscript) helps construct API queries for files. Useful for testing and learning parameters. ([dev.fandom.com][10])
* **DownloadImages** (Node.js) — a community script that downloads all images from a Fandom wiki via the API. Good starting point to adapt. ([dev.fandom.com][11])

# Politeness, pacing, and licensing

* **Rate & etiquette:** The MediaWiki API is designed for programmatic access, but keep it slow and steady (e.g., ~1 request/sec with sleeps, and honor `continue`). Community discussions suggest fairly generous edit limits for bots/users, but treat them as upper bounds and stay well under; back off on `429` or timeouts. ([dev.fandom.com][12])
* **Asset host:** Images download from `static.wikia.nocookie.net` (cookie-less CDN for files). Use the original `imageinfo` URL, not a resized thumbnail. ([community.fandom.com][4])
* **Licensing:** Fandom text is generally **CC-BY-SA** (“Community content is available under CC-BY-SA unless otherwise noted”), but **images may carry different licenses** (often not CC-BY-SA). If you’re keeping a collection, save each file’s description page URL and credit/author fields for compliance. ([Re:Zero Wiki][13])

# Quick curl examples

* First 3 files alphabetically:

```
curl 'https://rezero.fandom.com/api.php?action=query&list=allimages&ailimit=3&format=json'
```

* Get original URLs for two files:

```
curl 'https://rezero.fandom.com/api.php?action=query&prop=imageinfo&titles=File:Rem.png|File:Emilia.jpg&iiprop=url|mime|size&format=json'
```

* All files in a category (by title), then follow with `imageinfo`:

```
curl 'https://rezero.fandom.com/api.php?action=query&list=categorymembers&cmtitle=Category:Image_Gallery&cmtype=file&cmlimit=500&format=json'
```

---

If you tell me your exact scope (“everything”, or e.g. “only the official key art / artbook scans / game CGs”), I’ll tailor the API queries (categories, namespace filters, filetype/mime filters) and hand you a ready-to-run script that paces itself over weeks.
